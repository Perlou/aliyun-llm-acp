# ğŸ’» å¤§æ¨¡å‹åº”ç”¨å¼€å‘

> ğŸ“Š **è€ƒè¯•å æ¯”**: 16% (~16 é¢˜)
>
> ğŸ¯ **é‡è¦ç¨‹åº¦**: â­â­â­â­

## ğŸ“š çŸ¥è¯†å¤§çº²

### 1. å¤§æ¨¡å‹ API åŸºç¡€

#### 1.1 OpenAI å…¼å®¹ API æ ¼å¼

```python
from openai import OpenAI

client = OpenAI(
    api_key="your_api_key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

response = client.chat.completions.create(
    model="qwen-max",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½"}
    ]
)
```

#### 1.2 æ ¸å¿ƒ API å‚æ•° â­

| å‚æ•°          | ç±»å‹  | è¯´æ˜         | å¸¸è§å€¼                |
| ------------- | ----- | ------------ | --------------------- |
| `model`       | str   | æ¨¡å‹åç§°     | qwen-max, qwen-plus   |
| `messages`    | list  | å¯¹è¯å†å²     | system/user/assistant |
| `temperature` | float | éšæœºæ€§       | 0-2, é»˜è®¤ 1           |
| `top_p`       | float | æ ¸é‡‡æ ·       | 0-1, é»˜è®¤ 1           |
| `max_tokens`  | int   | æœ€å¤§è¾“å‡ºé•¿åº¦ | æŒ‰éœ€è®¾ç½®              |
| `stream`      | bool  | æµå¼è¾“å‡º     | true/false            |
| `stop`        | list  | åœæ­¢è¯       | è‡ªå®šä¹‰                |

#### 1.3 æ¶ˆæ¯è§’è‰²ç±»å‹

```python
messages = [
    {
        "role": "system",     # ç³»ç»Ÿè®¾å®šï¼Œè´¯ç©¿å¯¹è¯
        "content": "ä½ æ˜¯åŒ»ç–—åŠ©æ‰‹"
    },
    {
        "role": "user",       # ç”¨æˆ·è¾“å…¥
        "content": "ä»€ä¹ˆæ˜¯æ„Ÿå†’ï¼Ÿ"
    },
    {
        "role": "assistant",  # AI å›å¤
        "content": "æ„Ÿå†’æ˜¯..."
    }
]
```

### 2. æ‰¹é‡ç”Ÿæˆ vs æµå¼ç”Ÿæˆ â­

#### 2.1 æ‰¹é‡ç”Ÿæˆ (Non-streaming)

```python
# ç­‰å¾…å®Œæ•´å“åº”
response = client.chat.completions.create(
    model="qwen-max",
    messages=messages,
    stream=False  # é»˜è®¤
)

print(response.choices[0].message.content)
```

**ç‰¹ç‚¹**:

- ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç»“æœ
- ç”¨æˆ·éœ€ç­‰å¾…å®Œæ•´ç”Ÿæˆ
- é€‚åˆåå°å¤„ç†ä»»åŠ¡

#### 2.2 æµå¼ç”Ÿæˆ (Streaming)

```python
# é€å­—è¿”å›
response = client.chat.completions.create(
    model="qwen-max",
    messages=messages,
    stream=True  # å¼€å¯æµå¼
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

**ç‰¹ç‚¹**:

- å®æ—¶é€å­—è¿”å›
- ç”¨æˆ·ä½“éªŒæ›´å¥½
- é€‚åˆå¯¹è¯äº¤äº’åœºæ™¯

### 3. å¯¹è¯å†å²ç®¡ç†

#### 3.1 å¤šè½®å¯¹è¯å®ç°

```python
class ChatBot:
    def __init__(self, system_prompt):
        self.history = [
            {"role": "system", "content": system_prompt}
        ]

    def chat(self, user_input):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.history.append({"role": "user", "content": user_input})

        # è°ƒç”¨ API
        response = client.chat.completions.create(
            model="qwen-max",
            messages=self.history
        )

        # è·å–å›å¤
        assistant_msg = response.choices[0].message.content

        # ä¿å­˜åˆ°å†å²
        self.history.append({"role": "assistant", "content": assistant_msg})

        return assistant_msg
```

#### 3.2 ä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†

```python
def truncate_history(messages, max_tokens=4000):
    """ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼Œé¿å…è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶"""
    # ä¿ç•™ system prompt
    system = messages[0] if messages[0]["role"] == "system" else None

    # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹ä¿ç•™
    truncated = []
    total_tokens = 0

    for msg in reversed(messages[1:]):
        msg_tokens = len(msg["content"]) // 2  # ç²—ç•¥ä¼°ç®—
        if total_tokens + msg_tokens > max_tokens:
            break
        truncated.insert(0, msg)
        total_tokens += msg_tokens

    if system:
        truncated.insert(0, system)

    return truncated
```

### 4. LangChain åŸºç¡€

#### 4.1 æ ¸å¿ƒç»„ä»¶

```
LangChain æ¶æ„:
â”œâ”€â”€ Models (æ¨¡å‹å±‚)
â”‚   â”œâ”€â”€ LLMs: æ–‡æœ¬è¾“å…¥ â†’ æ–‡æœ¬è¾“å‡º
â”‚   â””â”€â”€ Chat Models: æ¶ˆæ¯è¾“å…¥ â†’ æ¶ˆæ¯è¾“å‡º
â”œâ”€â”€ Prompts (æç¤ºå±‚)
â”‚   â”œâ”€â”€ PromptTemplate: æç¤ºæ¨¡æ¿
â”‚   â””â”€â”€ ChatPromptTemplate: å¯¹è¯æ¨¡æ¿
â”œâ”€â”€ Chains (é“¾è·¯å±‚)
â”‚   â”œâ”€â”€ LLMChain: åŸºç¡€é“¾
â”‚   â””â”€â”€ SequentialChain: é¡ºåºé“¾
â”œâ”€â”€ Memory (è®°å¿†å±‚)
â”‚   â”œâ”€â”€ ConversationBufferMemory
â”‚   â””â”€â”€ ConversationSummaryMemory
â””â”€â”€ Agents (ä»£ç†å±‚)
    â””â”€â”€ ReAct Agent
```

#### 4.2 ç®€å• Chain ç¤ºä¾‹

```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import Tongyi

# åˆ›å»º LLM
llm = Tongyi(model_name="qwen-max")

# åˆ›å»ºæ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["topic"],
    template="è¯·å†™ä¸€ç¯‡å…³äº{topic}çš„ç®€çŸ­ä»‹ç»"
)

# åˆ›å»º Chain
chain = LLMChain(llm=llm, prompt=prompt)

# è¿è¡Œ
result = chain.run(topic="äººå·¥æ™ºèƒ½")
```

#### 4.3 Memory ä½¿ç”¨

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm,
    memory=memory
)

# å¯¹è¯ä¼šè‡ªåŠ¨è®°ä½å†å²
conversation.predict(input="ä½ å¥½")
conversation.predict(input="æˆ‘åˆšæ‰è¯´äº†ä»€ä¹ˆ?")
```

### 5. Dify å¹³å°

#### 5.1 Dify ç‰¹ç‚¹

- å¯è§†åŒ–ä½ä»£ç å¼€å‘
- æ‹–æ‹½å¼å·¥ä½œæµç¼–æ’
- å†…ç½® RAG å’Œ Agent
- å¤šæ¨¡å‹ç®¡ç†

#### 5.2 é€‚ç”¨åœºæ™¯

- å¿«é€ŸåŸå‹å¼€å‘
- éæŠ€æœ¯äººå‘˜ä½¿ç”¨
- å¤æ‚å·¥ä½œæµç¼–æ’

### 6. ç™¾ç‚¼å¹³å°åº”ç”¨å¼€å‘

#### 6.1 åº”ç”¨ç±»å‹

| ç±»å‹         | è¯´æ˜           | é€‚ç”¨åœºæ™¯       |
| ------------ | -------------- | -------------- |
| **æ™ºèƒ½é—®ç­”** | åŸºäºçŸ¥è¯†åº“é—®ç­” | å®¢æœã€æ–‡æ¡£åŠ©æ‰‹ |
| **æ™ºèƒ½ä½“**   | å·¥å…·è°ƒç”¨èƒ½åŠ›   | å¤æ‚ä»»åŠ¡å¤„ç†   |
| **å·¥ä½œæµ**   | å¤šæ­¥éª¤ç¼–æ’     | ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ– |

#### 6.2 API è°ƒç”¨ç¤ºä¾‹

```python
from dashscope import Application

response = Application.call(
    app_id="your_app_id",
    prompt="ç”¨æˆ·é—®é¢˜"
)

print(response.output.text)
```

---

## âœ… çŸ¥è¯†ç‚¹è‡ªæµ‹

1. [ ] messages ä¸­ä¸‰ç§ role çš„ä½œç”¨?
2. [ ] æµå¼ç”Ÿæˆå’Œæ‰¹é‡ç”Ÿæˆçš„åŒºåˆ«?
3. [ ] temperature å’Œ top_p å¦‚ä½•å½±å“è¾“å‡º?
4. [ ] LangChain çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›?
5. [ ] å¦‚ä½•ç®¡ç†å¯¹è¯å†å²é¿å…è¶…å‡ºä¸Šä¸‹æ–‡?

---

## ğŸ“ è€ƒç‚¹é€Ÿè®°å¡

```
ğŸ”¹ role ç±»å‹ = system + user + assistant
ğŸ”¹ stream=True â†’ é€å­—è¿”å›ï¼Œä½“éªŒå¥½
ğŸ”¹ temperature â†“ = ç¡®å®šæ€§ â†‘
ğŸ”¹ top_p = æ ¸é‡‡æ ·ï¼Œé€šå¸¸ 0.9
ğŸ”¹ LangChain = Models + Prompts + Chains + Memory
ğŸ”¹ å¤šè½®å¯¹è¯ = ç»´æŠ¤ messages åˆ—è¡¨
```
